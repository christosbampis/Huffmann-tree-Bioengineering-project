Στατιστικές μέθοδοι συμπίεσης ECG

Κυριότερο χαρακτηριστικό των στατιστικών μεθόδων συμπίεσης είναι το γεγονός ότι δημιουργούν κωδικοποιήσεις μεταβλητού μήκους σε συνδυασμό με την μη παραμορφωτική τους λειτουργία. Η ανάγκη για κωδικοποίηση μεταβλητού μήκους ανακύπτει όταν αντιληφθεί κανείς 2 βασικές έννοιες που σχετίζονται άμεσα με τον κόσμο των πιθανοτήτων, αυτήν της εντροπίας και εκείνη του πλεονασμού (entropy και redundancy στη ξενόγλωσση βιβλιογραφία). 

Υποθέτουμε αρχικά ότι έχουμε ένα βασικό αλφάβητο μεγέθους N από σύμβολα (α_1,α_2,…,α_N) όπου N ∈ Z εξαιρώντας το 0. Έστω, ακόμη, ότι αυτά τα σύμβολα εμφανίζονται στο ECG (αλλά και γενικότερα σε κάθε σήμα) με γνωστές πιθανότητες (p_1,p_2,…,p_N). Με τον όρο εντροπία ορίζουμε το -∑_(i=1)^N▒〖p_i  log_2⁡〖p_i 〗 〗. Η εντροπία αναπαριστά το μικρότερο αριθμό, κατά μέσο όρο, από bits ανά σύμβολο που χρειάζονται για την κωδικοποίηση του. Όταν όλες οι πιθανότητες εμφάνισης των συμβόλων που υπάρχουν στο αλφάβητο είναι ίσες (έστω k) τότε

-∑_(i=1)^N▒〖p_i  log_2⁡〖p_i 〗 〗=-∑_(i=1)^N▒〖k log_2⁡k 〗=-k log_2⁡∏_(i=1)^N▒k=-k log_2⁡〖k^N 〗=-kN log_2⁡k


Επειδή όμως στο σύμπαν (το αλφάβητο εν προκειμένω) που ορίστηκε πρέπει το άθροισμα των πιθανοτήτων όλων των συμβόλων να είναι ίσο με 1 (βεβαιότητα) τότε έχουμε ∑_(i=1)^N▒k=1⟹k=1/Ν⟺Ν=1/k.
Άρα, τελικά, -∑_(i=1)^N▒〖p_i  log_2⁡〖p_i 〗 〗=-log_2⁡〖1/Ν〗=log_2⁡N. Η διαφορά μεταξύ της εντροπίας του μηνύματος για ίσες πιθανότητες και της αντίστοιχης εντροπίας για την περίπτωση της μη ισοκατανομής των πιθανοτήτων, ονομάζεται πλεονασμός, δηλαδή R=log_2⁡N-(-∑_(i=1)^N▒〖p_i  log_2⁡〖p_i 〗 〗)=log_2⁡N+∑_(i=1)^N▒〖p_i  log_2⁡〖p_i 〗 〗

Αυτή ακριβώς η σχέση οδηγεί και στο να χρησιμοποιηθούνε κώδικες μεταβλητού μήκους για κάθε σύμβολο με στόχο την μονοσήμαντη αποκωδικοποίηση τους στο δέκτη και την επίτευξη καλής συμπίεσης με όσο το δυνατόν μικρότερη παραμόρφωση. Για το λόγο αυτό τα σύμβολα που έχουν μεγάλη πιθανότητα να εμφανιστούν κωδικοποιούνται με λιγότερα bits
σε σχέση με τα υπόλοιπα που εμφανίζονται πιο αραιά. Υπάρχουν διάφοροι αλγόριθμοι για τέτοιου είδους επιλογές με πρώτο (από ιστορικής πλευράς) τον κώδικα Morse και ακολουθούν άλλοι όπως ο Shannon - Fano, RLE, Huffman, Adaptive Huffman, Golomb code κλπ. Παρόμοιας λογικής αλλά διαφορετικής κατεύθυνσης (μιας και αποτελεί γενίκευση του Huffman) είναι και η έννοια του Arithmetic coding, η οποία θα αναλυθεί στη συνέχεια ξεχωριστά. 






Run Length Encoding (RLE)
	Η κωδικοποίηση RLE θεωρείται από τις πιο απλές μορφές συμπίεσης μιας και προϋποθέτει ότι τα δεδομένα θα εμφανίζονται σε ομάδες δηλαδή στη μορφή
WWWWWAAAKKKKKBBBBBBBBBBWCC
όπου τα W, A, K, B, C είναι κάποια σύμβολα (για το παράδειγμα του ECG μπορούμε να τα φανταστούμε σαν τιμές σε mV του πλάτους του σήματος).
Στην περίπτωση αυτή, διαδοχικά σύμβολα κωδικοποιούνται ενιαία, οπότε για το παραπάνω παράδειγμα παίρνουμε εύκολα
4W3A5K10B1W2C
με την έκφραση αυτή να συνιστά την προς μετάδοση ακολουθία. Επειδή ακριβώς, αυτή η μορφή δεδομένων δεν εμφανίζεται σε όλες τις εφαρμογές, η RLE κωδικοποίηση εμφανίζεται κυρίως σε αντίστοιχες εφαρμογές (fax, απλής μορφής γραφικά) και επικουρικά σε συνδυασμό με άλλες μεθόδους. Για τη συγκεκριμένη περίπτωση του ECG δεν αντιμετωπίζεται σαν μια καλή επιλογή συμπίεσης από μόνη της οπότε και δεν προτιμάται. Αντίστοιχη υλοποίηση που περιλαμβάνει την κωδικοποίηση RLE αλλά στο τελικό στάδιο έχει προηγουμένως αναλυθεί.
Shannon - Fano coding
	Η κωδικοποίηση Shannon - Fano αποτέλεσε μια πρωτόλεια μορφή στατιστικής συμπίεσης με απλά χαρακτηριστικά υλοποίησης αλλά φτωχά αποτελέσματα. Συγκεκριμένα, ενώ η κωδικοποίηση Huffman αποδίδει το βέλτιστο μήκος κώδικα, ελαχιστοποιώντας την επιβάρυνση αποθήκευσης και μετάδοσης, η κωδικοποίηση κατά Shannon - Fano δεν αποδίδει τόσο μικρούς  κώδικες. Για το λόγο αυτό, δεν χρησιμοποιείται ευρέως στην πράξη. Τα βήματα του αλγορίθμου είναι τα εξής:
	Δημιούργησε πιθανότητες συμβόλων
	Ταξινόμησε σε 2 υποσύνολα α, β όπου το άθροισμα των πιθανοτήτων των στοιχείων του πρώτου υποσυνόλου να είναι περίπου ίδιο με το δεύτερο υποσύνολο
	Εκχώρησε 0 για το υποσύνολο α και 1 για το υποσύνολο β
	Επανέλαβε ώσπου να μην υπάρχουν άλλα στοιχεία
Huffman coding
Στις εφαρμογές συμπίεσης του ECG κυρίως εμφανίζεται ο Huffman κώδικας με τις παραλλαγές του και συνήθως εφαρμόζεται σε τελικό στάδιο συμπίεσης του ECG (για παράδειγμα στη συμπίεση στατικών εικόνων JPEG) μετά από την εφαρμογή άλλων μεθόδων συμπίεσης οι οποίες έχουν εισάγει κάποιο λάθος σε σχέση με το αρχικό ECG. Αξίζει να σημειωθεί ότι ο κώδικας Huffman δεν εισάγει περαιτέρω λάθη, είναι ,δηλαδή, lossless τρόπος συμπίεσης.

Προτού γίνει συγκεκριμένη αναφορά στον τρόπο εφαρμογής στη συμπίεση ενός ECG, είναι σημαντικό να τονιστούν τα βασικά σημεία του αλγορίθμου. Αρχικά, οι είσοδοι θα είναι το αλφάβητο των συμβόλων και οι αντίστοιχες πιθανότητες εμφάνισης καθενός από αυτά. Εν συνεχεία, γίνεται μια φθίνουσα ταξινόμηση των συμβόλων ανάλογα με την πιθανότητας εμφάνισης τους. Έπειτα, γίνονται διαδοχικά ενοποιήσεις των 2 συμβόλων που κάθε φορά έχουν τις μικρότερες πιθανότητες εμφάνισης. Η ενοποίηση κάθε ζευγαριού δίνει ένα νέο σύμβολο με αντίστοιχη πιθανότητα εμφάνισης το άθροισμα των επιμέρους πιθανοτήτων των συμβόλων που το συνθέτουν. Κατά παραδοχή, σημειώνεται με 1 το σύμβολο που έχει τη μεγαλύτερη πιθανότητα από τα 2 που θα ενοποιηθούν και αντίστοιχα με 0 το άλλο. Τελικά, σχηματίζεται ένα δέντρο από 0 και 1 και καταλήγουμε σε ένα σύμβολο «ρίζα» που αποτελείται από όλα τα σύμβολα του αλφαβήτου. Στο σημείο αυτό αξίζει να σημειωθεί μια ενδιαφέρουσα διαφορά μεταξύ Huffman και Shannon – Fano: το δέντρο Shannon – Fano κατασκευάζεται από τη ρίζα προς τα φύλλα ενώ στην κωδικοποίηση Huffman κινούμαστε κατά την αντίθετη κατεύθυνση.

 

Χαρακτηριστικό παράδειγμα Huffman κώδικα από το βιβλίο 
David Solomon Data Compression



	Για τη διαδικασία της αποκωδικοποίησης ακολουθούμε την ανάποδη πορεία. Ξεκινώντας από τη ρίζα του δέντρου, ακολουθούμε κάθε φορά το «παιδί» που αντιστοιχεί στο bit που έχουμε λάβει. Με διαδοχικές προσπελάσεις του δέντρου φτάνουμε στα φύλλα αυτού τα οποία και σηματοδοτούν το σύμβολο που αρχικά στάλθηκε. Επαναλαμβάνουμε τη διαδικασία ώσπου να μην υπάρχουν άλλα μη επεξεργασμένα bits. Η διαδικασία κωδικοποίησης – αποκωδικοποίησης δεν θα πρέπει να εισάγει αμφισημίες ως προς την επεξεργασία συμβόλου, διότι τότε δεν μπορούμε να αποφασίσουμε ντετερμινιστικά το ποιο ήταν το εκπεμφθέν σύμβολο στον πομπό.
 
 
Χαρακτηριστικό παράδειγμα αλφαβήτου και δέντρου Huffman αποτελεί το λατινικό αλφάβητο με γνωστές πιθανότητες εμφάνισης συμβόλων:
 
Σημειώνεται ότι το ύψος του δέντρου όπως αυτό εμφανίζεται, δείχνει το μέγιστο αριθμό bits που απαιτείται για 1 σύμβολο, δηλαδή ότι σίγουρα κάποιο σύμβολο χρειάζεται τόσα bits (στο παράδειγμα αυτό 9) για να κωδικοποιηθεί. Όπως έχει προαναφερθεί, αναμένεται σύμβολα με τις πιο μικρές πιθανότητες εμφάνισης μεταξύ του συνόλου των συμβόλων να απαιτούν τέτοιο αριθμό από bits.
Στην περίπτωση αυτή, εμείς θα έχουμε τις τιμές του πλάτους που λαμβάνει το ECG και θα χρειαστεί με βάση αυτές να επιλέξουμε κάποιες κβαντισμένες στάθμες τιμών στις οποίες θα περιορίσουμε τις δυνατές τιμές του προκύπτοντος σήματος. Στην ουσία, όσα περισσότερα bit χρησιμοποιήσουμε για αυτήν την επιλογή, αναμένουμε καλύτερη ποιότητα στο τελικό σήμα, αλλά σαφώς χειρότερη (λιγότερη) συμπίεση. Η επιλογή αυτή είναι απαραίτητη για την υλοποίηση του αλγορίθμου, αφού απαιτείται να υπάρχει ένα πεπερασμένο σύνολο τιμών-συμβόλων πάνω στις οποίες θα εφαρμοστεί η προαναφερθείσα διαδικασία. Αναμένουμε η συμπίεση που θα προκύψει να βρίσκεται μεταξύ 2 χαρακτηριστικών τιμών: αυτήν της εντροπίας που χαρακτηρίζει την ιδανική συμπίεση και εκείνη της μηδενικής συμπίεσης. Για την εύρεση της πιθανότητας εμφάνισης συμβόλου υιοθετείται η προσέγγιση ότι όσες φορές εμφανιστεί το εν λόγω σύμβολο σε αυτό το δείγμα ECG αυτή θα είναι και η στατιστική πιθανότητα εμφάνισης του.
 

 
 
 
Επομένως, παρατηρείται ότι εκμεταλλευόμαστε τις διαφορετικές πιθανότητες εμφάνισης συμβόλων και επιτυγχάνεται συμπίεση κοντά σε εκείνη της εντροπίας. Για ανάλυση 8 bit έχουμε συμπίεση περίπου 0.7 δηλαδή χρειαζόμαστε το 70% των αρχικών bit. Το αποτέλεσμα αυτό συμφωνεί και με πειραματικά δεδομένα (http://www.dspguide.com/ch27/3.htm) από το διαδίκτυο αλλά και από τη θεωρία που δεν προβλέπει αποτελεσματική συμπίεση με όρους πρακτικών εφαρμογών, γι’ αυτό άλλωστε και ο Huffman δε χρησιμοποιείται ως ο κυρίαρχος τρόπος για τη συμπίεση.
Επίσης, παρατηρείται ότι από τα 4 bit και πέρα η κωδικοποίηση ασφαλώς και χαρακτηρίζεται ως κωδικοποίηση εντροπίας αφού τείνουν οι 2 γραμμές να ταυτιστούν. Οι 2 τελευταίες τιμές του bestCR δεν απεικονίζονται αφού κάποια σύμβολα δεν εμφανίστηκαν οπότε ο λογάριθμος παίρνει μη πεπερασμένη τιμή. Το PRD προφανώς θα μειώνεται όσο αυξάνουμε τα bit ανάλυσης που χρησιμοποιούμε αφού τότε όλο και περισσότερο θα προσεγγίζονται με μεγαλύτερη ακρίβεια οι τιμές του αρχικού σήματος ECG.
Στο σημείο αυτό αξίζει να σημειωθούν κάποιες αδυναμίες της κωδικοποίησης Huffman όπως αυτή παρουσιάστηκε μέχρι στιγμής. Πρώτον, η επίδοση της κωδικοποίησης Huffman επηρεάζεται από τη στατιστική φύση των δεδομένων με αποτέλεσμα να μην επιτυγχάνει πάντα την καλύτερη δυνατή μη παραμορφωτική συμπίεση (εντροπία). Το δεύτερο στοιχείο είναι ότι χρειαζόμαστε να ξέρουμε από πριν τις πιθανότητες εμφάνισης των συμβόλων, γεγονός μη εφικτό στην πράξη. Στη συνέχεια αναλύονται 2 τεχνικές που επεκτείνουν τη ιδέα της κωδικοποίησης Huffman και αντιμετωπίζουν αυτές τις αδυναμίες.
Adaptive Huffman coding
	Στην κωδικοποίηση Huffman που προηγήθηκε θεωρήσαμε ότι είναι εξαρχής γνωστές οι πιθανότητες εμφάνισης των συμβόλων του αλφαβήτου. Μια τέτοια θεώρηση είναι βολική για να καταστρώσουμε μια εύληπτη υλοποίηση, όμως δεν αντανακλά πλήρως την πραγματικότητα. Στην πράξη, είναι σχεδόν απίθανο να γνωρίζουμε τις πιθανότητες αυτές και μάλιστα η οποιαδήποτε γνώση τους μπορεί να είναι και παραπλανητική. Για παράδειγμα, σε μια βάση δεδομένων όπου έχουμε αποθηκεύσει χιλιάδες ECG ή σε μια εφαρμογή επιτόπου καταγραφής ECG (real - time) δε μπορούμε να έχουμε περιορισμένες επιλογές του ποια είναι τα σύμβολα, πόσο μάλλον του πόσο συχνά αυτά εμφανίζονται. Για το λόγο αυτό, χρειάζεται η προσαρμοστική κωδικοποίηση Huffman όπου:
	Ξεκινάμε με ένα άδειο δέντρο Huffman
	Για το 1ο σύμβολο που διαβάζεται το προωθούμε στη ροή εξόδου (output stream) δίχως να το κωδικοποιήσουμε
	Το σύμβολο προστίθεται στο δέντρο και του αποδίδεται ένας κώδικας
	Για κάθε ίδιο σύμβολο με το προαναφερθέν αυξάνει η πιθανότητα εμφάνισης του  αντίστοιχου συμβόλου κατά 1
	Αν η αλλαγή στην πιθανότητα εμφάνισης αναδιατάσσει τους κόμβους του δέντρου, ελέγχεται αν ικανοποιείται εκ νέου η συνθήκη δέντρου Huffman. Αν όχι, τροποποιείται το δέντρο ώστε αυτή να πληρείται και προσαρμόζεται ο κώδικας
Στην πλευρά του δέκτη απαιτείται η αντίστροφη διαδικασία, για την οποία βέβαια προϋποτίθεται η γνώση του αν πρόκειται για μη κωδικοποιημένο σύμβολο ή όχι. Προς τούτο, προστίθεται ένας ειδικός κώδικας διαφυγής (escape code) ώστε να «σημαδεύεται» το μη κωδικοποιημένο σύμβολο. Καθώς προχωράει η διαδικασία αποκωδικοποίησης και το δέντρο αλλάζει, μετακινείται κατάλληλα αυτός ο κώδικας ώστε να αντιστοιχεί εκ νέου στο μη κωδικοποιημένο σύμβολο. Στο σημείο αυτό χρειάζεται να τονιστεί ότι και στον πομπό και στον δέκτη σχηματίζονται 2 δέντρα τα οποία ενώ κατά τη διάρκεια της εφαρμογής δεν είναι τα ίδια (μιας και δεν είναι δυνατή η ταυτόχρονη κωδικοποίηση και αποκωδικοποίηση) τελικά ο σχηματισμός τους είναι πανομοιότυπος αλλά ασύγχρονος. Για την πλήρη αποσαφήνιση της διαδικασίας παρατίθενται 1 συνοπτικό διάγραμμα ροής που απεικονίζει την τροποποίηση του δέντρου από τον κωδικοποιητή στην εκπομπή και τον αποκωδικοποιητή στη μεριά της λήψης.

 
Arithmetic coding
	Όπως προαναφέρθηκε, η κωδικοποίηση Huffman (στατική ή προσαρμοστική), έχει πολύ καλά αποτελέσματα σε γενικές γραμμές επιτυγχάνοντας εύστοχη στατιστική αποδόση bits. Όμως, επειδή ακριβώς αποδίδει ακέραιο αριθμό bits σε κάθε σύμβολο (1 ή 2 για παράδειγμα) δεν μπορεί πάντα να δώσει τον ελάχιστο αριθμό, μιας και αυτός μπορεί να μην είναι ακέραιος! Ένα παράδειγμα μπορεί να είναι πιο διαφωτιστικό:
Έστω ένα αλφάβητο Χ που αποτελείται κάποια σύμβολα, μεταξύ των οποίων και το a με πιθανότητα εμφάνισης 0.4. Ο ελάχιστος αριθμός bits ισούται με περίπου 〖- log〗_2 (0.4)=1.32 bits τα οποία προφανώς δεν είναι ακέραιος αριθμός. Επομένως, η κωδικοποίηση Huffman αποδίδει τον ελάχιστο αριθμό bits όταν οι πιθανότητες εμφάνισης είναι της μορφής 2^(-κ),κ=1,2,κλπ. Με το Arithmetic coding ξεπερνάμε αυτήν τη δυσκολία αίροντας την απόδοση ακεραίου αριθμού bits σε κάθε σύμβολο γενικεύοντας την κατά μήκος του αρχείου με τον ακόλουθο τρόπο.
 
	Συνοψίζοντας την κατά τα άλλα σχετικά πιο πολύπλοκη κωδικοποίηση σε σχέση με την Huffman, η βασική τους διαφορά είναι ότι δεν κωδικοποιείται κάθε σύμβολο χωριστά, αλλά το σύνολο του μηνύματος κωδικοποιείται σε έναν αριθμό μεταξύ 0 και 1 ο οποίος περιγράφει το τελευταίο υποδιάστημα που προέκυψε κατά την κατάτμηση του αρχικού διαστήματος [0,1). Η κατάτμηση αυτή γίνεται με βάση τις πιθανότητες του συμβόλου που κάθε φορά διαβάζεται με αποτέλεσμα και εδώ να προαπαιτείται η γνώση της στατιστικής φύσης των δεδομένων. Εναλλακτικά, υπάρχουν πολλές προσαρμοστικές τροποποιήσεις και επιλογές στην περίπτωση που κάτι τέτοιο δεν είναι εφικτό.
	Συμπερασματικά, από τη μελέτη των στατιστικών μεθόδων αντλήθηκαν τα εξής συμπεράσματα (επιγραμματικά):
	Σχηματίζουμε κώδικες μεταβλητού μήκους ανάλογα της στατιστικής φύσης των δεδομένων
	Πρόκειται για μη σύνολο μη παραμορφωτικών μεθόδων με την εμφάνιση, όμως, περιορισμών στην απόδοση τους (εντροπία)
	Η αδυναμία πρότερου προσδιορισμού της στατιστικής φύσης των δεδομένων οδηγεί σε πιο σύνθετες λύσεις – τις προσαρμοστικές.
	Μεταξύ άλλων, η κωδικοποίηση Huffman δίνει ικανοποιητικά αποτελέσματα, τα οποία επαρκούν για τις εφαρμογές συνεπικουρούμενης συμπίεσης (εκεί δηλαδή όπου εμφανίζεται σαν τελικό στάδιο).
	Το Arithmetic coding μπορεί να επιτύχει τη βέλτιστη λύση.
Βιβλιογραφία
David Salomon  Data Compression
The Data Compression Book - Mark Nelson - (2nd Edition)
http://www.dspguide.com/ch27/3.htm
http://en.wikipedia.org/wiki/Huffman_coding
http://www.siggraph.org/education/materials/HyperGraph/video/mpeg/mpegfaq/huffman_tutorial.html
http://www.cs.duke.edu/csed/curious/compression/adaptivehuff.html#tree
http://en.wikipedia.org/wiki/Arithmetic_coding#Huffman_coding
http://everything2.com/title/statistical+compression
http://en.wikipedia.org/wiki/Entropy_%28information_theory%29
